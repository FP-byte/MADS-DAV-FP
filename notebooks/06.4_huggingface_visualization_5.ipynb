{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-DAV/blob/main/notebooks/6.4_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use transfer learning to extract more types of features from our texts. There are a few things to consider when taking this approach:\n",
    "\n",
    "- the models are pretrained on a corpus of texts. This can have a lot of impact; consider for example the difference between a \"sentiment\" model trained on movie reviews and one trained on tweets. The might grasps sort of the same concept, but the words used and the way they are used are different. If you are going to use this model to estimate the sentiment of, let's say, emails sent in a business context, you might get unexpected results.\n",
    "- There are really a lot of different models on huggingface. It can be usefull to browse around in the model hub to see what is available, and try to find something that is close to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/huggingface/transformers.git\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this the first time, it will download the model from huggingface hub.\n",
    "The second run will be much faster. You will get three outputs: positive, neutral and negative. The sum of these three is 1, because it is a probability distribution.\n",
    "\n",
    "It would be straightforward to use this on your own dataset. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>message</th>\n",
       "      <th>author</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>language</th>\n",
       "      <th>date</th>\n",
       "      <th>isoweek</th>\n",
       "      <th>year-week</th>\n",
       "      <th>hour</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-26 09:43:00</td>\n",
       "      <td>Is het goed als ik me ziekmeld maar dat ik nog...</td>\n",
       "      <td>effervescent-camel</td>\n",
       "      <td>False</td>\n",
       "      <td>NL</td>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>9</td>\n",
       "      <td>kom thuis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-26 09:44:00</td>\n",
       "      <td>Kom maar naar huis</td>\n",
       "      <td>hilarious-goldfinch</td>\n",
       "      <td>False</td>\n",
       "      <td>NL</td>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>9</td>\n",
       "      <td>kom thuis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-26 09:45:00</td>\n",
       "      <td>Ja dit uur is om 10u afgelopen ga dan wel naar...</td>\n",
       "      <td>effervescent-camel</td>\n",
       "      <td>False</td>\n",
       "      <td>NL</td>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>9</td>\n",
       "      <td>kom thuis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-29 21:07:00</td>\n",
       "      <td>Irene a che ora torni a casa</td>\n",
       "      <td>nimble-wombat</td>\n",
       "      <td>False</td>\n",
       "      <td>IT</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>21</td>\n",
       "      <td>kom thuis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-29 23:04:00</td>\n",
       "      <td>En thuis hoor papa</td>\n",
       "      <td>effervescent-camel</td>\n",
       "      <td>False</td>\n",
       "      <td>NL</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>23</td>\n",
       "      <td>kom thuis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp                                            message  \\\n",
       "0 2019-03-26 09:43:00  Is het goed als ik me ziekmeld maar dat ik nog...   \n",
       "1 2019-03-26 09:44:00                                 Kom maar naar huis   \n",
       "2 2019-03-26 09:45:00  Ja dit uur is om 10u afgelopen ga dan wel naar...   \n",
       "3 2019-03-29 21:07:00                       Irene a che ora torni a casa   \n",
       "4 2019-03-29 23:04:00                                 En thuis hoor papa   \n",
       "\n",
       "                author  has_emoji language        date  isoweek year-week  \\\n",
       "0   effervescent-camel      False       NL  2019-03-26       13   2019-12   \n",
       "1  hilarious-goldfinch      False       NL  2019-03-26       13   2019-12   \n",
       "2   effervescent-camel      False       NL  2019-03-26       13   2019-12   \n",
       "3        nimble-wombat      False       IT  2019-03-29       13   2019-12   \n",
       "4   effervescent-camel      False       NL  2019-03-29       13   2019-12   \n",
       "\n",
       "   hour      topic  \n",
       "0     9  kom thuis  \n",
       "1     9  kom thuis  \n",
       "2     9  kom thuis  \n",
       "3    21  kom thuis  \n",
       "4    23  kom thuis  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tomllib\n",
    "configfile = Path(\"../config.toml\").resolve()\n",
    "with configfile.open(\"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "datafile = (Path(\"../\") / Path(config[\"processed\"]) / config[\"current\"]).resolve()\n",
    "if not datafile.exists():\n",
    "    logger.warning(\"Datafile does not exist. First run src/preprocess.py, and check the timestamp!\")\n",
    "df = pd.read_parquet(datafile)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5123/5123 [00:00<00:00, 18813.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Author:\n",
    "    name: str\n",
    "    alltext: str\n",
    "    chunked: list[str]\n",
    "    sentiment: dict = None\n",
    "\n",
    "# extract the data from the dataframe\n",
    "datadict = {}\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    author = row[\"author\"]\n",
    "    message = row[\"message\"]\n",
    "    if author not in datadict:\n",
    "        datadict[author] = Author(name=author, alltext=message, chunked=[])\n",
    "    else:\n",
    "        datadict[author].alltext += message\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_size=512):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Create chunks of the specified size\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# For every author, try to split their text into chunks of 512 tokens\n",
    "for author in datadict:\n",
    "    # Get the combined text for the author\n",
    "    text = datadict[author].alltext\n",
    "    # Split the text into chunks\n",
    "    # we want 512 tokens, so lets guess about 50%\n",
    "    datadict[author].chunked = split_into_chunks(text, chunk_size=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping text to a semantic vectorspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install light-embed\n",
    "#!pip install sentence-transformers==2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of mapping the result of the non-linear transformations to just one dimension of sentiment, we can pick a more general model that doesnt do this. This model is \"just\" trained on a lot of textual data, and the output vectors will represent the meaning of the text in a high dimensional space. This can be used to compare the meaning of different texts, or to use as input for a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      2\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach sentence is converted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we input two sentences, and the output in this case are two vectors, 384 dimensions each. \n",
    "\n",
    "I will try to filter the text that are too short (eg more than just \"hi\" and \"hello\") to see if we can get a bit more interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_length'] = df['message'].str.len()\n",
    "sns.histplot(x=np.log(df[\"message_length\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/message_length.png\" width=450 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My dataset seems to have a median message length of log(x) = 4, so lets take 5 as a cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[np.log(df[\"message_length\"]) > 3].reset_index(drop=True)\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a new class to keep metadata and the output neatly together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Embedding:\n",
    "    metadata: list\n",
    "    vectors: np.ndarray\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return (self.vectors[idx], self.metadata[idx])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Embedding, dims={self.vectors.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "metadata = {}\n",
    "text = []\n",
    "for idx, row in tqdm(subset.iterrows(), total=len(subset)):\n",
    "    author = row[\"author\"]\n",
    "    message = row[\"message\"]\n",
    "    timestamp = row[\"timestamp\"]\n",
    "    metadata[idx] = {\"author\": author, \"message\": message, \"timestamp\": timestamp}\n",
    "    text.append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the model to encode every message. \n",
    "if the length of your text is not too big, this will work in one go\n",
    "otherwise, you might want to split the text into smaller chunks, encode the chunks,\n",
    "and then concatenate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.encode(text)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And store it in our dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(metadata, vectors)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our `__getitem__` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = emb[1]\n",
    "X.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualise this would be with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(emb.vectors)\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = [emb.metadata[i][\"author\"] for i in range(len(emb.metadata))]\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels)\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tSNE is often better for visualising high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X = tsne.fit_transform(emb.vectors)\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = [emb.metadata[i][\"author\"] for i in range(len(emb.metadata))]\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels)\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be the case that you just get a blob of points with no clear clustering. Some things to consider:\n",
    "\n",
    "- you might have just too much authors, and if you look more closely you might still find that some authors are more similar than others\n",
    "- you text messages might overall be too short and too similar. You might need to filter out more messages, or group messages of the same author together and then encode them.\n",
    "- you might want to add more structure. Eg, label some messages by hand (or with a regex) and use that as a coloring. You might find that some type of message actually do cluster together in a relevant way, just not clustered by author but more by subject.\n",
    "\n",
    "Please keep in mind that normally, when doing unsupervised clustering, you will have some idea of what you are looking for. For example, you might be looking for fraud, or you are looking for a certain sentiment, or for a specific topic. A typical strategy would be to hand-label a few items and then calculate the distance to find \"close\" items you didn't label yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_author = {}\n",
    "for i in range(len(emb)):\n",
    "    # for every embedding\n",
    "    X, y = emb[i]\n",
    "    # we store the embedding in a list per author, and average it later\n",
    "    avg_author[y[\"author\"]] = avg_author.get(y[\"author\"], []) + [X]\n",
    "\n",
    "for author, vectors in avg_author.items():\n",
    "    # take the average of all embeddings per author\n",
    "    avg_author[author] = np.mean(vectors, axis=0)\n",
    "# We extract all values as a single matrix\n",
    "A = np.array(list(avg_author.values()))\n",
    "labels = list(avg_author.keys())\n",
    "A.shape, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(A, yticklabels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the distance between the average vector for every author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "D = distance_matrix(A, A)\n",
    "sns.heatmap(D, yticklabels=labels, xticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reduce the \"distance\" fingerprint to two dimensions and plot it.\n",
    "This will show us which authors are in a similar way close to other authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(D)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels)\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
